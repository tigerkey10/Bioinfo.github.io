{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn, einsum\n",
    "from einops import rearrange, repeat\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# feedforward and attention\n",
    "class GEGLU(nn.Module):\n",
    "    def forward(self, x):\n",
    "        x, gates = x.chunk(2, dim = -1)\n",
    "        return x * F.gelu(gates)\n",
    "\n",
    "def FeedForward(dim, mult = 4, dropout = 0.):\n",
    "    return nn.Sequential(\n",
    "        nn.LayerNorm(dim),\n",
    "        nn.Linear(dim, dim * mult * 2),\n",
    "        GEGLU(),\n",
    "        nn.Dropout(dropout),\n",
    "        nn.Linear(dim * mult, dim)\n",
    "    )\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim,\n",
    "        heads = 8,\n",
    "        dim_head = 64,\n",
    "        dropout = 0.\n",
    "    ):\n",
    "        super().__init__()\n",
    "        inner_dim = dim_head * heads\n",
    "        self.heads = heads\n",
    "        self.scale = dim_head ** -0.5\n",
    "\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "\n",
    "        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias = False)\n",
    "        self.to_out = nn.Linear(inner_dim, dim, bias = False)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.heads\n",
    "\n",
    "        x = self.norm(x)\n",
    "\n",
    "        q, k, v = self.to_qkv(x).chunk(3, dim = -1)\n",
    "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h = h), (q, k, v))\n",
    "        q = q * self.scale\n",
    "\n",
    "        sim = einsum('b h i d, b h j d -> b h i j', q, k)\n",
    "\n",
    "        attn = sim.softmax(dim = -1)\n",
    "        attn = self.dropout(attn)\n",
    "\n",
    "        out = einsum('b h i j, b h j d -> b h i d', attn, v)\n",
    "        out = rearrange(out, 'b h n d -> b n (h d)', h = h)\n",
    "        return self.to_out(out)\n",
    "\n",
    "# transformer\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim,\n",
    "        depth,\n",
    "        heads,\n",
    "        dim_head,\n",
    "        attn_dropout,\n",
    "        ff_dropout\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([])\n",
    "\n",
    "        for _ in range(depth):\n",
    "            self.layers.append(nn.ModuleList([\n",
    "                Attention(dim, heads = heads, dim_head = dim_head, dropout = attn_dropout),\n",
    "                FeedForward(dim, dropout = ff_dropout),\n",
    "            ]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        for attn, ff in self.layers:\n",
    "            x = attn(x) + x\n",
    "            x = ff(x) + x\n",
    "\n",
    "        return x\n",
    "\n",
    "# numerical embedder\n",
    "class NumericalEmbedder(nn.Module):\n",
    "    def __init__(self, dim, num_numerical_types):\n",
    "        super().__init__()\n",
    "        self.weights = nn.Parameter(torch.randn(num_numerical_types, dim))\n",
    "        self.biases = nn.Parameter(torch.randn(num_numerical_types, dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = rearrange(x, 'b n -> b n 1')\n",
    "        return x * self.weights + self.biases\n",
    "\n",
    "# model \n",
    "class network(nn.Module) : \n",
    "    def __init__(\n",
    "        self,\n",
    "        categories,\n",
    "        num_continuous,\n",
    "        dim,\n",
    "        depth,\n",
    "        heads,\n",
    "        dim_head = 16,\n",
    "        dim_out = 1,\n",
    "        num_special_tokens = 1, # CLS\n",
    "        attn_dropout = 0.,\n",
    "        ff_dropout = 0.\n",
    "    ):\n",
    "        super(network, self).__init__()\n",
    "        assert all(map(lambda n: n > 0, categories)), 'number of each category must be positive'\n",
    "\n",
    "        # categories related calculations\n",
    "        self.num_categories = len(categories)\n",
    "        self.num_unique_categories = sum(categories)\n",
    "\n",
    "        # create category embeddings table\n",
    "        self.num_special_tokens = num_special_tokens\n",
    "        total_tokens = self.num_unique_categories + num_special_tokens\n",
    "\n",
    "        # for automatically offsetting unique category ids to the correct position in the categories embedding table\n",
    "        categories_offset = F.pad(torch.tensor(list(categories)), (1, 0), value = num_special_tokens)\n",
    "        categories_offset = categories_offset.cumsum(dim = -1)[:-1]\n",
    "        self.register_buffer('categories_offset', categories_offset)\n",
    "\n",
    "        # categorical embedding\n",
    "        self.categorical_embeds = nn.Embedding(total_tokens, dim)\n",
    "\n",
    "        # continuous\n",
    "        self.numerical_embedder = NumericalEmbedder(dim, num_continuous)\n",
    "\n",
    "        # cls token\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, dim))\n",
    "\n",
    "        # transformer\n",
    "        self.transformer = Transformer(            \n",
    "            dim = dim,\n",
    "            depth = depth,\n",
    "            heads = heads,\n",
    "            dim_head = dim_head,\n",
    "            attn_dropout = attn_dropout,\n",
    "            ff_dropout = ff_dropout\n",
    "        )\n",
    "        \n",
    "        self.linear_stack = nn.Sequential(\n",
    "            nn.LayerNorm(dim),\n",
    "            nn.ReLU(), \n",
    "            nn.Linear(dim, dim_out),\n",
    "            nn.Sigmoid())\n",
    "\n",
    "    def forward(self, x_categ, x_numer):\n",
    "        b = x_categ.shape[0]\n",
    "\n",
    "        assert x_categ.shape[-1] == self.num_categories, f'you must pass in {self.num_categories} values for your categories input'\n",
    "        x_categ += self.categories_offset\n",
    "\n",
    "        x_categ = self.categorical_embeds(x_categ)\n",
    "\n",
    "        # add numerically embedded tokens\n",
    "\n",
    "        x_numer = self.numerical_embedder(x_numer)\n",
    "\n",
    "        # concat categorical and numerical\n",
    "\n",
    "        x = torch.cat((x_categ, x_numer), dim = 1)\n",
    "\n",
    "        # append cls tokens\n",
    "\n",
    "        cls_tokens = repeat(self.cls_token, '1 1 d -> b 1 d', b = b)\n",
    "        x = torch.cat((cls_tokens, x), dim = 1)\n",
    "\n",
    "        # attend\n",
    "\n",
    "        x = self.transformer(x)\n",
    "\n",
    "        # get cls token\n",
    "\n",
    "        x = x[:, 0]\n",
    "\n",
    "        return self.linear_stack(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim # call for optimizer\n",
    "from torchvision import datasets, transforms # for dataset processing \n",
    "\n",
    "is_cuda = torch.cuda.is_available() ; is_cuda # gpu 사용가능한가 확인 # true\n",
    "device = torch.device('cuda' if is_cuda else 'cpu') # device(type='cuda'). cpu 또는 gpu 로 장비 사용 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameter 지정 \n",
    "batch_size = 50 # 미니배치 사이즈 \n",
    "epoch_num = 800 # epoch 수 \n",
    "learning_rate = 0.001 # step size (=learning rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>height(cm)</th>\n",
       "      <th>weight(kg)</th>\n",
       "      <th>waist(cm)</th>\n",
       "      <th>eyesight(left)</th>\n",
       "      <th>eyesight(right)</th>\n",
       "      <th>hearing(left)</th>\n",
       "      <th>hearing(right)</th>\n",
       "      <th>...</th>\n",
       "      <th>hemoglobin</th>\n",
       "      <th>Urine protein</th>\n",
       "      <th>serum creatinine</th>\n",
       "      <th>AST</th>\n",
       "      <th>ALT</th>\n",
       "      <th>Gtp</th>\n",
       "      <th>oral</th>\n",
       "      <th>dental caries</th>\n",
       "      <th>tartar</th>\n",
       "      <th>smoking</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>F</td>\n",
       "      <td>40</td>\n",
       "      <td>155</td>\n",
       "      <td>60</td>\n",
       "      <td>81.3</td>\n",
       "      <td>1.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>12.9</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.7</td>\n",
       "      <td>18.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>Y</td>\n",
       "      <td>0</td>\n",
       "      <td>Y</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>F</td>\n",
       "      <td>40</td>\n",
       "      <td>160</td>\n",
       "      <td>60</td>\n",
       "      <td>81.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>12.7</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.6</td>\n",
       "      <td>22.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>Y</td>\n",
       "      <td>0</td>\n",
       "      <td>Y</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>M</td>\n",
       "      <td>55</td>\n",
       "      <td>170</td>\n",
       "      <td>60</td>\n",
       "      <td>80.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>15.8</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>Y</td>\n",
       "      <td>0</td>\n",
       "      <td>N</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>M</td>\n",
       "      <td>40</td>\n",
       "      <td>165</td>\n",
       "      <td>70</td>\n",
       "      <td>88.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>14.7</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>Y</td>\n",
       "      <td>0</td>\n",
       "      <td>Y</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>F</td>\n",
       "      <td>40</td>\n",
       "      <td>155</td>\n",
       "      <td>60</td>\n",
       "      <td>86.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>12.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.6</td>\n",
       "      <td>16.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>Y</td>\n",
       "      <td>0</td>\n",
       "      <td>N</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55687</th>\n",
       "      <td>55676</td>\n",
       "      <td>F</td>\n",
       "      <td>40</td>\n",
       "      <td>170</td>\n",
       "      <td>65</td>\n",
       "      <td>75.0</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>12.3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.6</td>\n",
       "      <td>14.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>Y</td>\n",
       "      <td>1</td>\n",
       "      <td>Y</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55688</th>\n",
       "      <td>55681</td>\n",
       "      <td>F</td>\n",
       "      <td>45</td>\n",
       "      <td>160</td>\n",
       "      <td>50</td>\n",
       "      <td>70.0</td>\n",
       "      <td>1.2</td>\n",
       "      <td>1.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>14.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.9</td>\n",
       "      <td>20.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>Y</td>\n",
       "      <td>0</td>\n",
       "      <td>Y</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55689</th>\n",
       "      <td>55683</td>\n",
       "      <td>F</td>\n",
       "      <td>55</td>\n",
       "      <td>160</td>\n",
       "      <td>50</td>\n",
       "      <td>68.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>12.4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>17.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>Y</td>\n",
       "      <td>0</td>\n",
       "      <td>N</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55690</th>\n",
       "      <td>55684</td>\n",
       "      <td>M</td>\n",
       "      <td>60</td>\n",
       "      <td>165</td>\n",
       "      <td>60</td>\n",
       "      <td>78.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>14.4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.7</td>\n",
       "      <td>20.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>Y</td>\n",
       "      <td>0</td>\n",
       "      <td>N</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55691</th>\n",
       "      <td>55691</td>\n",
       "      <td>M</td>\n",
       "      <td>55</td>\n",
       "      <td>160</td>\n",
       "      <td>65</td>\n",
       "      <td>85.0</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.7</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>26.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>Y</td>\n",
       "      <td>0</td>\n",
       "      <td>Y</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>55692 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          ID gender  age  height(cm)  weight(kg)  waist(cm)  eyesight(left)  \\\n",
       "0          0      F   40         155          60       81.3             1.2   \n",
       "1          1      F   40         160          60       81.0             0.8   \n",
       "2          2      M   55         170          60       80.0             0.8   \n",
       "3          3      M   40         165          70       88.0             1.5   \n",
       "4          4      F   40         155          60       86.0             1.0   \n",
       "...      ...    ...  ...         ...         ...        ...             ...   \n",
       "55687  55676      F   40         170          65       75.0             0.9   \n",
       "55688  55681      F   45         160          50       70.0             1.2   \n",
       "55689  55683      F   55         160          50       68.5             1.0   \n",
       "55690  55684      M   60         165          60       78.0             0.8   \n",
       "55691  55691      M   55         160          65       85.0             0.9   \n",
       "\n",
       "       eyesight(right)  hearing(left)  hearing(right)  ...  hemoglobin  \\\n",
       "0                  1.0            1.0             1.0  ...        12.9   \n",
       "1                  0.6            1.0             1.0  ...        12.7   \n",
       "2                  0.8            1.0             1.0  ...        15.8   \n",
       "3                  1.5            1.0             1.0  ...        14.7   \n",
       "4                  1.0            1.0             1.0  ...        12.5   \n",
       "...                ...            ...             ...  ...         ...   \n",
       "55687              0.9            1.0             1.0  ...        12.3   \n",
       "55688              1.2            1.0             1.0  ...        14.0   \n",
       "55689              1.2            1.0             1.0  ...        12.4   \n",
       "55690              1.0            1.0             1.0  ...        14.4   \n",
       "55691              0.7            1.0             1.0  ...        15.0   \n",
       "\n",
       "       Urine protein  serum creatinine   AST   ALT   Gtp  oral  dental caries  \\\n",
       "0                1.0               0.7  18.0  19.0  27.0     Y              0   \n",
       "1                1.0               0.6  22.0  19.0  18.0     Y              0   \n",
       "2                1.0               1.0  21.0  16.0  22.0     Y              0   \n",
       "3                1.0               1.0  19.0  26.0  18.0     Y              0   \n",
       "4                1.0               0.6  16.0  14.0  22.0     Y              0   \n",
       "...              ...               ...   ...   ...   ...   ...            ...   \n",
       "55687            1.0               0.6  14.0   7.0  10.0     Y              1   \n",
       "55688            1.0               0.9  20.0  12.0  14.0     Y              0   \n",
       "55689            1.0               0.5  17.0  11.0  12.0     Y              0   \n",
       "55690            1.0               0.7  20.0  19.0  18.0     Y              0   \n",
       "55691            1.0               0.8  26.0  29.0  41.0     Y              0   \n",
       "\n",
       "       tartar  smoking  \n",
       "0           Y        0  \n",
       "1           Y        0  \n",
       "2           N        1  \n",
       "3           Y        0  \n",
       "4           N        0  \n",
       "...       ...      ...  \n",
       "55687       Y        0  \n",
       "55688       Y        0  \n",
       "55689       N        0  \n",
       "55690       N        0  \n",
       "55691       Y        1  \n",
       "\n",
       "[55692 rows x 27 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 데이터셋 로드 \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "smoking = pd.read_csv('/Users/user/Desktop/smoking.csv') ; smoking "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "smoking.drop(columns=['ID', 'gender', 'height(cm)','hearing(right)', 'hearing(left)', 'eyesight(right)', 'eyesight(left)', 'Urine protein'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "smoking.drop(columns=['oral'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "smoking = pd.get_dummies(smoking, drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler \n",
    "\n",
    "mm = MinMaxScaler()\n",
    "scaler = ['age','weight(kg)', 'waist(cm)', 'systolic',\n",
    "       'relaxation', 'fasting blood sugar', 'Cholesterol', 'triglyceride',\n",
    "       'HDL', 'LDL', 'hemoglobin','serum creatinine', 'AST',\n",
    "       'ALT', 'Gtp']\n",
    "\n",
    "mm.fit(smoking[scaler])\n",
    "smoking[scaler] = mm.transform(smoking[scaler])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = smoking.drop(columns=['smoking']).values\n",
    "y = smoking['smoking'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_x, test_x, train_target_x, test_target_x = train_test_split(X, y, test_size=0.2, random_state=156)\n",
    "test_x, valid_x, test_target_x, valid_target_x = train_test_split(test_x, test_target_x, test_size=0.2, random_state=156)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(44553, 17)\n",
      "(8911, 17)\n",
      "(2228, 17)\n"
     ]
    }
   ],
   "source": [
    "print(train_x.shape)\n",
    "print(test_x.shape)\n",
    "print(valid_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train x\n",
    "x_categ_train = train_x[:, [-1, -2]]\n",
    "x_numer_train = train_x[:, : -2]\n",
    "\n",
    "# test x\n",
    "x_categ_test = test_x[:, [-1, -2]]\n",
    "x_numer_test = test_x[:, : -2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# valid x\n",
    "x_categ_valid = valid_x[:, [-1, -2]]\n",
    "x_numer_valid = valid_x[:, : -2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 실험\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "# Dataset 상속\n",
    "import numpy as np \n",
    "class CustomDataset(Dataset): \n",
    "    def __init__(self):\n",
    "        self.x_data = x_categ_train\n",
    "        self.x2_data = x_numer_train\n",
    "        self.y_data = train_target_x\n",
    "\n",
    "  # 총 데이터의 개수를 리턴\n",
    "    def __len__(self): \n",
    "        return len(self.x_data)\n",
    "\n",
    "  # 인덱스를 입력받아 그에 맵핑되는 입출력 데이터를 파이토치의 Tensor 형태로 리턴\n",
    "    def __getitem__(self, idx): \n",
    "        x = torch.Tensor(self.x_data[idx]) # categ\n",
    "        x2 = torch.Tensor(self.x2_data[idx]) # numeric\n",
    "        y = torch.Tensor(train_target_x.reshape(len(train_target_x), 1))[idx]\n",
    "        return (x, x2), y\n",
    "\n",
    "dataset = CustomDataset()\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = network(categories=(2,2), \n",
    "       num_continuous=15, \n",
    "       dim = 32, \n",
    "       depth=8, # transformer layer 수\n",
    "       heads=8, attn_dropout=0.2, ff_dropout=0.2).to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train 함수 정의 \n",
    "def train(model, dataloader, optimizer, criterion, device) : \n",
    "    model.train()\n",
    "    epoch_loss = 0 \n",
    "    for batch_idx, samples in enumerate(dataloader) : \n",
    "        x_train, y_train = samples \n",
    "        x_categ, x_numer = x_train # tuple 언패킹\n",
    "        x_categ = x_categ.type(torch.IntTensor)\n",
    "        \n",
    "        x_categ = x_categ.to(device)\n",
    "        x_numer = x_numer.to(device)\n",
    "        y_train = y_train.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model(x_categ, x_numer)\n",
    "        loss = criterion(output, y_train) # batch 에 대한 loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item() # 이번 epoch 에서 loss 총합\n",
    "        \n",
    "    return epoch_loss / len(dataloader) # 이번 epoch 에서 loss 평균 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트셋 구성\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "# Dataset 상속\n",
    "import numpy as np \n",
    "class Customtestset(Dataset): \n",
    "    def __init__(self):\n",
    "        self.x_data = x_categ_test\n",
    "        self.x2_data = x_numer_test\n",
    "        self.y_data = test_target_x\n",
    "\n",
    "  # 총 데이터의 개수를 리턴\n",
    "    def __len__(self): \n",
    "        return len(self.x_data)\n",
    "\n",
    "  # 인덱스를 입력받아 그에 맵핑되는 입출력 데이터를 파이토치의 Tensor 형태로 리턴\n",
    "    def __getitem__(self, idx): \n",
    "        x = torch.Tensor(self.x_data[idx]) # categ\n",
    "        x2 = torch.Tensor(self.x2_data[idx]) # numeric\n",
    "        y = torch.Tensor(self.y_data.reshape(len(self.y_data), 1))[idx]\n",
    "        return (x, x2), y\n",
    "    \n",
    "testset = Customtestset()\n",
    "testloader = DataLoader(testset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트셋에서 모델 성능 평가 함수 , 평가지표 accuracy rate 사용.\n",
    "def evaluate(model, testloader, device) : \n",
    "    model.eval()  \n",
    "    error = 0\n",
    "    with torch.no_grad() : \n",
    "        for batch_idx, samples in enumerate(testloader) : \n",
    "            x_test, y_test = samples\n",
    "            x_categ_test, x_numer_test = x_test # tuple 언패킹\n",
    "            x_categ_test = x_categ_test.type(torch.IntTensor)\n",
    "\n",
    "            x_categ_test = x_categ_test.to(device)\n",
    "            x_numer_test = x_numer_test.to(device)\n",
    "            y_test = y_test.to(device)\n",
    "\n",
    "            output = model(x_categ_test, x_numer_test)\n",
    "            prediction = torch.Tensor([0 if x <= 0.5 else 1 for x in output.data]).to(device)\n",
    "            error += prediction.ne(torch.flatten(y_test)).sum().item() # 이번 Epoch 에서 틀린 것 총 갯수 누적 \n",
    "            \n",
    "    return error/len(testloader.dataset) # 이번 Epoch 에서 error rate, accuracy rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "Train loss: 0.551\n",
      "Test loss: 0.287\n",
      "Epoch: 1\n",
      "Train loss: 0.528\n",
      "Test loss: 0.292\n",
      "Epoch: 2\n",
      "Train loss: 0.522\n",
      "Test loss: 0.281\n",
      "Epoch: 3\n",
      "Train loss: 0.518\n",
      "Test loss: 0.273\n",
      "Epoch: 4\n",
      "Train loss: 0.514\n",
      "Test loss: 0.285\n",
      "Epoch: 5\n",
      "Train loss: 0.513\n",
      "Test loss: 0.273\n",
      "Epoch: 6\n",
      "Train loss: 0.511\n",
      "Test loss: 0.276\n",
      "Epoch: 7\n",
      "Train loss: 0.509\n",
      "Test loss: 0.268\n",
      "Epoch: 8\n",
      "Train loss: 0.506\n",
      "Test loss: 0.266\n",
      "Epoch: 9\n",
      "Train loss: 0.505\n",
      "Test loss: 0.267\n",
      "Epoch: 10\n",
      "Train loss: 0.502\n",
      "Test loss: 0.288\n",
      "Epoch: 11\n",
      "Train loss: 0.501\n",
      "Test loss: 0.273\n",
      "Epoch: 12\n",
      "Train loss: 0.499\n",
      "Test loss: 0.261\n",
      "Epoch: 13\n",
      "Train loss: 0.498\n",
      "Test loss: 0.259\n",
      "Epoch: 14\n",
      "Train loss: 0.498\n",
      "Test loss: 0.264\n",
      "Epoch: 15\n",
      "Train loss: 0.496\n",
      "Test loss: 0.259\n",
      "Epoch: 16\n",
      "Train loss: 0.495\n",
      "Test loss: 0.26\n",
      "Epoch: 17\n",
      "Train loss: 0.495\n",
      "Test loss: 0.258\n",
      "Epoch: 18\n",
      "Train loss: 0.494\n",
      "Test loss: 0.27\n",
      "Epoch: 19\n",
      "Train loss: 0.494\n",
      "Test loss: 0.268\n",
      "Epoch: 20\n",
      "Train loss: 0.492\n",
      "Test loss: 0.26\n",
      "Epoch: 21\n",
      "Train loss: 0.493\n",
      "Test loss: 0.252\n",
      "Epoch: 22\n",
      "Train loss: 0.492\n",
      "Test loss: 0.257\n",
      "Epoch: 23\n",
      "Train loss: 0.494\n",
      "Test loss: 0.262\n",
      "Epoch: 24\n",
      "Train loss: 0.492\n",
      "Test loss: 0.259\n",
      "Epoch: 25\n",
      "Train loss: 0.491\n",
      "Test loss: 0.256\n",
      "Epoch: 26\n",
      "Train loss: 0.491\n",
      "Test loss: 0.257\n",
      "Epoch: 27\n",
      "Train loss: 0.49\n",
      "Test loss: 0.256\n",
      "Epoch: 28\n",
      "Train loss: 0.488\n",
      "Test loss: 0.257\n",
      "Epoch: 29\n",
      "Train loss: 0.489\n",
      "Test loss: 0.258\n",
      "Epoch: 30\n",
      "Train loss: 0.491\n",
      "Test loss: 0.275\n",
      "Epoch: 31\n",
      "Train loss: 0.49\n",
      "Test loss: 0.259\n",
      "Epoch: 32\n",
      "Train loss: 0.491\n",
      "Test loss: 0.265\n",
      "Epoch: 33\n",
      "Train loss: 0.491\n",
      "Test loss: 0.254\n",
      "Epoch: 34\n",
      "Train loss: 0.489\n",
      "Test loss: 0.26\n",
      "Epoch: 35\n",
      "Train loss: 0.49\n",
      "Test loss: 0.25\n",
      "Epoch: 36\n",
      "Train loss: 0.489\n",
      "Test loss: 0.253\n",
      "Epoch: 37\n",
      "Train loss: 0.487\n",
      "Test loss: 0.263\n",
      "Epoch: 38\n",
      "Train loss: 0.487\n",
      "Test loss: 0.258\n",
      "Epoch: 39\n",
      "Train loss: 0.488\n",
      "Test loss: 0.262\n",
      "Epoch: 40\n",
      "Train loss: 0.486\n",
      "Test loss: 0.256\n",
      "Epoch: 41\n",
      "Train loss: 0.486\n",
      "Test loss: 0.255\n",
      "Epoch: 42\n",
      "Train loss: 0.487\n",
      "Test loss: 0.255\n",
      "Epoch: 43\n",
      "Train loss: 0.486\n",
      "Test loss: 0.259\n",
      "Epoch: 44\n",
      "Train loss: 0.486\n",
      "Test loss: 0.257\n",
      "Epoch: 45\n",
      "Train loss: 0.487\n",
      "Test loss: 0.254\n",
      "Epoch: 46\n",
      "Train loss: 0.485\n",
      "Test loss: 0.255\n",
      "Epoch: 47\n",
      "Train loss: 0.486\n",
      "Test loss: 0.257\n",
      "Epoch: 48\n",
      "Train loss: 0.485\n",
      "Test loss: 0.259\n",
      "Epoch: 49\n",
      "Train loss: 0.485\n",
      "Test loss: 0.26\n",
      "Epoch: 50\n",
      "Train loss: 0.485\n",
      "Test loss: 0.253\n",
      "Epoch: 51\n",
      "Train loss: 0.484\n",
      "Test loss: 0.258\n",
      "Epoch: 52\n",
      "Train loss: 0.484\n",
      "Test loss: 0.258\n",
      "Epoch: 53\n",
      "Train loss: 0.484\n",
      "Test loss: 0.254\n",
      "Epoch: 54\n",
      "Train loss: 0.484\n",
      "Test loss: 0.256\n",
      "Epoch: 55\n",
      "Train loss: 0.483\n",
      "Test loss: 0.252\n",
      "Epoch: 56\n",
      "Train loss: 0.484\n",
      "Test loss: 0.253\n",
      "Epoch: 57\n",
      "Train loss: 0.483\n",
      "Test loss: 0.256\n",
      "Epoch: 58\n",
      "Train loss: 0.484\n",
      "Test loss: 0.256\n",
      "Epoch: 59\n",
      "Train loss: 0.484\n",
      "Test loss: 0.256\n",
      "Epoch: 60\n",
      "Train loss: 0.483\n",
      "Test loss: 0.257\n",
      "Epoch: 61\n",
      "Train loss: 0.482\n",
      "Test loss: 0.261\n",
      "Epoch: 62\n",
      "Train loss: 0.482\n",
      "Test loss: 0.259\n",
      "Epoch: 63\n",
      "Train loss: 0.483\n",
      "Test loss: 0.256\n",
      "Epoch: 64\n",
      "Train loss: 0.481\n",
      "Test loss: 0.259\n",
      "Epoch: 65\n",
      "Train loss: 0.482\n",
      "Test loss: 0.257\n",
      "Epoch: 66\n",
      "Train loss: 0.481\n",
      "Test loss: 0.254\n",
      "Epoch: 67\n",
      "Train loss: 0.48\n",
      "Test loss: 0.255\n",
      "Epoch: 68\n",
      "Train loss: 0.481\n",
      "Test loss: 0.253\n",
      "Epoch: 69\n",
      "Train loss: 0.48\n",
      "Test loss: 0.253\n",
      "Epoch: 70\n",
      "Train loss: 0.482\n",
      "Test loss: 0.258\n",
      "Epoch: 71\n",
      "Train loss: 0.482\n",
      "Test loss: 0.254\n",
      "Epoch: 72\n",
      "Train loss: 0.481\n",
      "Test loss: 0.253\n",
      "Epoch: 73\n",
      "Train loss: 0.48\n",
      "Test loss: 0.253\n",
      "Epoch: 74\n",
      "Train loss: 0.48\n",
      "Test loss: 0.253\n",
      "Epoch: 75\n",
      "Train loss: 0.479\n",
      "Test loss: 0.254\n",
      "Epoch: 76\n",
      "Train loss: 0.481\n",
      "Test loss: 0.254\n",
      "Epoch: 77\n",
      "Train loss: 0.48\n",
      "Test loss: 0.253\n",
      "Epoch: 78\n",
      "Train loss: 0.479\n",
      "Test loss: 0.266\n",
      "Epoch: 79\n",
      "Train loss: 0.481\n",
      "Test loss: 0.257\n",
      "Epoch: 80\n",
      "Train loss: 0.48\n",
      "Test loss: 0.254\n",
      "Epoch: 81\n",
      "Train loss: 0.48\n",
      "Test loss: 0.25\n",
      "Epoch: 82\n",
      "Train loss: 0.479\n",
      "Test loss: 0.255\n",
      "Epoch: 83\n",
      "Train loss: 0.48\n",
      "Test loss: 0.259\n",
      "Epoch: 84\n",
      "Train loss: 0.48\n",
      "Test loss: 0.25\n",
      "Epoch: 85\n",
      "Train loss: 0.478\n",
      "Test loss: 0.255\n",
      "Epoch: 86\n",
      "Train loss: 0.478\n",
      "Test loss: 0.249\n",
      "Epoch: 87\n",
      "Train loss: 0.48\n",
      "Test loss: 0.253\n",
      "Epoch: 88\n",
      "Train loss: 0.478\n",
      "Test loss: 0.254\n",
      "Epoch: 89\n",
      "Train loss: 0.478\n",
      "Test loss: 0.254\n",
      "Epoch: 90\n",
      "Train loss: 0.477\n",
      "Test loss: 0.255\n",
      "Epoch: 91\n",
      "Train loss: 0.478\n",
      "Test loss: 0.253\n",
      "Epoch: 92\n",
      "Train loss: 0.478\n",
      "Test loss: 0.256\n",
      "Epoch: 93\n",
      "Train loss: 0.48\n",
      "Test loss: 0.252\n",
      "Epoch: 94\n",
      "Train loss: 0.476\n",
      "Test loss: 0.252\n",
      "Epoch: 95\n",
      "Train loss: 0.476\n",
      "Test loss: 0.255\n",
      "Epoch: 96\n",
      "Train loss: 0.476\n",
      "Test loss: 0.257\n",
      "Epoch: 97\n",
      "Train loss: 0.476\n",
      "Test loss: 0.255\n",
      "Epoch: 98\n",
      "Train loss: 0.476\n",
      "Test loss: 0.252\n",
      "Epoch: 99\n",
      "Train loss: 0.475\n",
      "Test loss: 0.249\n",
      "Epoch: 100\n",
      "Train loss: 0.475\n",
      "Test loss: 0.25\n",
      "Epoch: 101\n",
      "Train loss: 0.475\n",
      "Test loss: 0.254\n",
      "Epoch: 102\n",
      "Train loss: 0.476\n",
      "Test loss: 0.255\n",
      "Epoch: 103\n",
      "Train loss: 0.476\n",
      "Test loss: 0.254\n",
      "Epoch: 104\n",
      "Train loss: 0.474\n",
      "Test loss: 0.25\n",
      "Epoch: 105\n",
      "Train loss: 0.474\n",
      "Test loss: 0.252\n",
      "Epoch: 106\n",
      "Train loss: 0.474\n",
      "Test loss: 0.25\n",
      "Epoch: 107\n",
      "Train loss: 0.475\n",
      "Test loss: 0.252\n",
      "Epoch: 108\n",
      "Train loss: 0.474\n",
      "Test loss: 0.254\n",
      "Epoch: 109\n",
      "Train loss: 0.473\n",
      "Test loss: 0.254\n",
      "Epoch: 110\n",
      "Train loss: 0.473\n",
      "Test loss: 0.25\n",
      "Epoch: 111\n",
      "Train loss: 0.473\n",
      "Test loss: 0.256\n",
      "Epoch: 112\n",
      "Train loss: 0.472\n",
      "Test loss: 0.25\n",
      "Epoch: 113\n",
      "Train loss: 0.474\n",
      "Test loss: 0.25\n",
      "Epoch: 114\n",
      "Train loss: 0.472\n",
      "Test loss: 0.252\n",
      "Epoch: 115\n",
      "Train loss: 0.472\n",
      "Test loss: 0.256\n",
      "Epoch: 116\n",
      "Train loss: 0.472\n",
      "Test loss: 0.253\n",
      "Epoch: 117\n",
      "Train loss: 0.473\n",
      "Test loss: 0.254\n",
      "Epoch: 118\n",
      "Train loss: 0.471\n",
      "Test loss: 0.252\n",
      "Epoch: 119\n",
      "Train loss: 0.471\n",
      "Test loss: 0.254\n",
      "Epoch: 120\n",
      "Train loss: 0.47\n",
      "Test loss: 0.26\n",
      "Epoch: 121\n",
      "Train loss: 0.47\n",
      "Test loss: 0.25\n",
      "Epoch: 122\n",
      "Train loss: 0.47\n",
      "Test loss: 0.255\n",
      "Epoch: 123\n",
      "Train loss: 0.47\n",
      "Test loss: 0.253\n",
      "Epoch: 124\n",
      "Train loss: 0.47\n",
      "Test loss: 0.253\n",
      "Epoch: 125\n",
      "Train loss: 0.47\n",
      "Test loss: 0.25\n",
      "Epoch: 126\n",
      "Train loss: 0.47\n",
      "Test loss: 0.252\n",
      "Epoch: 127\n",
      "Train loss: 0.469\n",
      "Test loss: 0.253\n",
      "Epoch: 128\n",
      "Train loss: 0.469\n",
      "Test loss: 0.254\n",
      "Epoch: 129\n",
      "Train loss: 0.469\n",
      "Test loss: 0.252\n",
      "Epoch: 130\n",
      "Train loss: 0.469\n",
      "Test loss: 0.247\n",
      "Epoch: 131\n",
      "Train loss: 0.468\n",
      "Test loss: 0.255\n",
      "Epoch: 132\n",
      "Train loss: 0.468\n",
      "Test loss: 0.248\n",
      "Epoch: 133\n",
      "Train loss: 0.468\n",
      "Test loss: 0.251\n",
      "Epoch: 134\n",
      "Train loss: 0.468\n",
      "Test loss: 0.251\n",
      "Epoch: 135\n",
      "Train loss: 0.466\n",
      "Test loss: 0.258\n",
      "Epoch: 136\n",
      "Train loss: 0.466\n",
      "Test loss: 0.256\n",
      "Epoch: 137\n",
      "Train loss: 0.467\n",
      "Test loss: 0.255\n",
      "Epoch: 138\n",
      "Train loss: 0.466\n",
      "Test loss: 0.254\n",
      "Epoch: 139\n",
      "Train loss: 0.465\n",
      "Test loss: 0.251\n",
      "Epoch: 140\n",
      "Train loss: 0.465\n",
      "Test loss: 0.253\n",
      "Epoch: 141\n",
      "Train loss: 0.466\n",
      "Test loss: 0.254\n",
      "Epoch: 142\n",
      "Train loss: 0.465\n",
      "Test loss: 0.253\n",
      "Epoch: 143\n",
      "Train loss: 0.464\n",
      "Test loss: 0.251\n",
      "Epoch: 144\n",
      "Train loss: 0.464\n",
      "Test loss: 0.25\n",
      "Epoch: 145\n",
      "Train loss: 0.463\n",
      "Test loss: 0.254\n",
      "Epoch: 146\n",
      "Train loss: 0.462\n",
      "Test loss: 0.256\n",
      "Epoch: 147\n",
      "Train loss: 0.464\n",
      "Test loss: 0.25\n",
      "Epoch: 148\n",
      "Train loss: 0.463\n",
      "Test loss: 0.254\n",
      "Epoch: 149\n",
      "Train loss: 0.463\n",
      "Test loss: 0.252\n",
      "Epoch: 150\n",
      "Train loss: 0.463\n",
      "Test loss: 0.251\n",
      "Epoch: 151\n",
      "Train loss: 0.462\n",
      "Test loss: 0.254\n",
      "Epoch: 152\n",
      "Train loss: 0.463\n",
      "Test loss: 0.252\n",
      "Epoch: 153\n",
      "Train loss: 0.461\n",
      "Test loss: 0.252\n",
      "Epoch: 154\n",
      "Train loss: 0.459\n",
      "Test loss: 0.256\n",
      "Epoch: 155\n",
      "Train loss: 0.46\n",
      "Test loss: 0.263\n",
      "Epoch: 156\n",
      "Train loss: 0.461\n",
      "Test loss: 0.253\n",
      "Epoch: 157\n",
      "Train loss: 0.461\n",
      "Test loss: 0.253\n",
      "Epoch: 158\n",
      "Train loss: 0.46\n",
      "Test loss: 0.251\n",
      "Epoch: 159\n",
      "Train loss: 0.46\n",
      "Test loss: 0.255\n",
      "Epoch: 160\n",
      "Train loss: 0.46\n",
      "Test loss: 0.257\n",
      "Epoch: 161\n",
      "Train loss: 0.458\n",
      "Test loss: 0.254\n",
      "Epoch: 162\n",
      "Train loss: 0.458\n",
      "Test loss: 0.258\n",
      "Epoch: 163\n",
      "Train loss: 0.457\n",
      "Test loss: 0.255\n",
      "Epoch: 164\n",
      "Train loss: 0.458\n",
      "Test loss: 0.253\n",
      "Epoch: 165\n",
      "Train loss: 0.456\n",
      "Test loss: 0.257\n",
      "Epoch: 166\n",
      "Train loss: 0.457\n",
      "Test loss: 0.254\n",
      "Epoch: 167\n",
      "Train loss: 0.456\n",
      "Test loss: 0.254\n",
      "Epoch: 168\n",
      "Train loss: 0.455\n",
      "Test loss: 0.253\n",
      "Epoch: 169\n",
      "Train loss: 0.458\n",
      "Test loss: 0.253\n",
      "Epoch: 170\n",
      "Train loss: 0.457\n",
      "Test loss: 0.259\n",
      "Epoch: 171\n",
      "Train loss: 0.454\n",
      "Test loss: 0.251\n",
      "Epoch: 172\n",
      "Train loss: 0.455\n",
      "Test loss: 0.249\n",
      "Epoch: 173\n",
      "Train loss: 0.455\n",
      "Test loss: 0.254\n",
      "Epoch: 174\n",
      "Train loss: 0.454\n",
      "Test loss: 0.252\n",
      "Epoch: 175\n",
      "Train loss: 0.453\n",
      "Test loss: 0.251\n",
      "Epoch: 176\n",
      "Train loss: 0.452\n",
      "Test loss: 0.252\n",
      "Epoch: 177\n",
      "Train loss: 0.452\n",
      "Test loss: 0.253\n",
      "Epoch: 178\n",
      "Train loss: 0.451\n",
      "Test loss: 0.251\n",
      "Epoch: 179\n",
      "Train loss: 0.451\n",
      "Test loss: 0.258\n",
      "Epoch: 180\n",
      "Train loss: 0.452\n",
      "Test loss: 0.25\n",
      "Epoch: 181\n",
      "Train loss: 0.45\n",
      "Test loss: 0.254\n",
      "Epoch: 182\n",
      "Train loss: 0.449\n",
      "Test loss: 0.252\n",
      "Epoch: 183\n",
      "Train loss: 0.448\n",
      "Test loss: 0.256\n",
      "Epoch: 184\n",
      "Train loss: 0.448\n",
      "Test loss: 0.252\n",
      "Epoch: 185\n",
      "Train loss: 0.448\n",
      "Test loss: 0.256\n",
      "Epoch: 186\n",
      "Train loss: 0.446\n",
      "Test loss: 0.259\n",
      "Epoch: 187\n",
      "Train loss: 0.448\n",
      "Test loss: 0.257\n",
      "Epoch: 188\n",
      "Train loss: 0.447\n",
      "Test loss: 0.255\n",
      "Epoch: 189\n",
      "Train loss: 0.445\n",
      "Test loss: 0.256\n",
      "Epoch: 190\n",
      "Train loss: 0.446\n",
      "Test loss: 0.256\n",
      "Epoch: 191\n",
      "Train loss: 0.444\n",
      "Test loss: 0.25\n",
      "Epoch: 192\n",
      "Train loss: 0.443\n",
      "Test loss: 0.256\n",
      "Epoch: 193\n",
      "Train loss: 0.444\n",
      "Test loss: 0.256\n",
      "Epoch: 194\n",
      "Train loss: 0.442\n",
      "Test loss: 0.251\n",
      "Epoch: 195\n",
      "Train loss: 0.442\n",
      "Test loss: 0.257\n",
      "Epoch: 196\n",
      "Train loss: 0.442\n",
      "Test loss: 0.253\n",
      "Epoch: 197\n",
      "Train loss: 0.44\n",
      "Test loss: 0.257\n",
      "Epoch: 198\n",
      "Train loss: 0.441\n",
      "Test loss: 0.256\n",
      "Epoch: 199\n",
      "Train loss: 0.44\n",
      "Test loss: 0.255\n",
      "Epoch: 200\n",
      "Train loss: 0.44\n",
      "Test loss: 0.254\n",
      "Epoch: 201\n",
      "Train loss: 0.438\n",
      "Test loss: 0.255\n",
      "Epoch: 202\n",
      "Train loss: 0.44\n",
      "Test loss: 0.259\n",
      "Epoch: 203\n",
      "Train loss: 0.437\n",
      "Test loss: 0.255\n",
      "Epoch: 204\n",
      "Train loss: 0.437\n",
      "Test loss: 0.253\n",
      "Epoch: 205\n",
      "Train loss: 0.435\n",
      "Test loss: 0.251\n",
      "Epoch: 206\n",
      "Train loss: 0.434\n",
      "Test loss: 0.255\n",
      "Epoch: 207\n",
      "Train loss: 0.433\n",
      "Test loss: 0.253\n",
      "Epoch: 208\n",
      "Train loss: 0.434\n",
      "Test loss: 0.255\n",
      "Epoch: 209\n",
      "Train loss: 0.431\n",
      "Test loss: 0.254\n",
      "Epoch: 210\n",
      "Train loss: 0.433\n",
      "Test loss: 0.258\n",
      "Epoch: 211\n",
      "Train loss: 0.433\n",
      "Test loss: 0.252\n",
      "Epoch: 212\n",
      "Train loss: 0.431\n",
      "Test loss: 0.258\n",
      "Epoch: 213\n",
      "Train loss: 0.43\n",
      "Test loss: 0.252\n",
      "Epoch: 214\n",
      "Train loss: 0.429\n",
      "Test loss: 0.256\n",
      "Epoch: 215\n",
      "Train loss: 0.428\n",
      "Test loss: 0.254\n",
      "Epoch: 216\n",
      "Train loss: 0.428\n",
      "Test loss: 0.255\n",
      "Epoch: 217\n",
      "Train loss: 0.427\n",
      "Test loss: 0.251\n",
      "Epoch: 218\n",
      "Train loss: 0.426\n",
      "Test loss: 0.251\n",
      "Epoch: 219\n",
      "Train loss: 0.426\n",
      "Test loss: 0.251\n",
      "Epoch: 220\n",
      "Train loss: 0.425\n",
      "Test loss: 0.253\n",
      "Epoch: 221\n",
      "Train loss: 0.422\n",
      "Test loss: 0.256\n",
      "Epoch: 222\n",
      "Train loss: 0.422\n",
      "Test loss: 0.253\n",
      "Epoch: 223\n",
      "Train loss: 0.421\n",
      "Test loss: 0.253\n",
      "Epoch: 224\n",
      "Train loss: 0.42\n",
      "Test loss: 0.253\n",
      "Epoch: 225\n",
      "Train loss: 0.419\n",
      "Test loss: 0.255\n",
      "Epoch: 226\n",
      "Train loss: 0.417\n",
      "Test loss: 0.251\n",
      "Epoch: 227\n",
      "Train loss: 0.419\n",
      "Test loss: 0.254\n",
      "Epoch: 228\n",
      "Train loss: 0.415\n",
      "Test loss: 0.25\n",
      "Epoch: 229\n",
      "Train loss: 0.416\n",
      "Test loss: 0.249\n",
      "Epoch: 230\n",
      "Train loss: 0.416\n",
      "Test loss: 0.254\n",
      "Epoch: 231\n",
      "Train loss: 0.415\n",
      "Test loss: 0.255\n",
      "Epoch: 232\n",
      "Train loss: 0.413\n",
      "Test loss: 0.25\n",
      "Epoch: 233\n",
      "Train loss: 0.414\n",
      "Test loss: 0.254\n",
      "Epoch: 234\n",
      "Train loss: 0.414\n",
      "Test loss: 0.249\n",
      "Epoch: 235\n",
      "Train loss: 0.412\n",
      "Test loss: 0.252\n",
      "Epoch: 236\n",
      "Train loss: 0.409\n",
      "Test loss: 0.248\n",
      "Epoch: 237\n",
      "Train loss: 0.407\n",
      "Test loss: 0.252\n",
      "Epoch: 238\n",
      "Train loss: 0.407\n",
      "Test loss: 0.251\n",
      "Epoch: 239\n",
      "Train loss: 0.404\n",
      "Test loss: 0.253\n",
      "Epoch: 240\n",
      "Train loss: 0.407\n",
      "Test loss: 0.247\n",
      "Epoch: 241\n",
      "Train loss: 0.405\n",
      "Test loss: 0.256\n",
      "Epoch: 242\n",
      "Train loss: 0.403\n",
      "Test loss: 0.248\n",
      "Epoch: 243\n",
      "Train loss: 0.401\n",
      "Test loss: 0.249\n",
      "Epoch: 244\n",
      "Train loss: 0.399\n",
      "Test loss: 0.255\n",
      "Epoch: 245\n",
      "Train loss: 0.401\n",
      "Test loss: 0.246\n",
      "Epoch: 246\n",
      "Train loss: 0.398\n",
      "Test loss: 0.245\n",
      "Epoch: 247\n",
      "Train loss: 0.397\n",
      "Test loss: 0.249\n",
      "Epoch: 248\n",
      "Train loss: 0.397\n",
      "Test loss: 0.255\n",
      "Epoch: 249\n",
      "Train loss: 0.397\n",
      "Test loss: 0.247\n",
      "Epoch: 250\n",
      "Train loss: 0.396\n",
      "Test loss: 0.247\n",
      "Epoch: 251\n",
      "Train loss: 0.394\n",
      "Test loss: 0.242\n",
      "Epoch: 252\n",
      "Train loss: 0.393\n",
      "Test loss: 0.244\n",
      "Epoch: 253\n",
      "Train loss: 0.393\n",
      "Test loss: 0.246\n",
      "Epoch: 254\n",
      "Train loss: 0.392\n",
      "Test loss: 0.254\n",
      "Epoch: 255\n",
      "Train loss: 0.391\n",
      "Test loss: 0.248\n",
      "Epoch: 256\n",
      "Train loss: 0.389\n",
      "Test loss: 0.249\n",
      "Epoch: 257\n",
      "Train loss: 0.388\n",
      "Test loss: 0.247\n",
      "Epoch: 258\n",
      "Train loss: 0.386\n",
      "Test loss: 0.251\n",
      "Epoch: 259\n",
      "Train loss: 0.387\n",
      "Test loss: 0.245\n",
      "Epoch: 260\n",
      "Train loss: 0.384\n",
      "Test loss: 0.239\n",
      "Epoch: 261\n",
      "Train loss: 0.383\n",
      "Test loss: 0.246\n",
      "Epoch: 262\n",
      "Train loss: 0.383\n",
      "Test loss: 0.248\n",
      "Epoch: 263\n",
      "Train loss: 0.381\n",
      "Test loss: 0.241\n",
      "Epoch: 264\n",
      "Train loss: 0.378\n",
      "Test loss: 0.242\n",
      "Epoch: 265\n",
      "Train loss: 0.379\n",
      "Test loss: 0.244\n",
      "Epoch: 266\n",
      "Train loss: 0.376\n",
      "Test loss: 0.247\n",
      "Epoch: 267\n",
      "Train loss: 0.378\n",
      "Test loss: 0.248\n",
      "Epoch: 268\n",
      "Train loss: 0.374\n",
      "Test loss: 0.246\n",
      "Epoch: 269\n",
      "Train loss: 0.374\n",
      "Test loss: 0.245\n",
      "Epoch: 270\n",
      "Train loss: 0.374\n",
      "Test loss: 0.241\n",
      "Epoch: 271\n",
      "Train loss: 0.371\n",
      "Test loss: 0.248\n",
      "Epoch: 272\n",
      "Train loss: 0.372\n",
      "Test loss: 0.247\n",
      "Epoch: 273\n",
      "Train loss: 0.367\n",
      "Test loss: 0.248\n",
      "Epoch: 274\n",
      "Train loss: 0.37\n",
      "Test loss: 0.241\n",
      "Epoch: 275\n",
      "Train loss: 0.367\n",
      "Test loss: 0.242\n",
      "Epoch: 276\n",
      "Train loss: 0.365\n",
      "Test loss: 0.244\n",
      "Epoch: 277\n",
      "Train loss: 0.366\n",
      "Test loss: 0.245\n",
      "Epoch: 278\n",
      "Train loss: 0.364\n",
      "Test loss: 0.246\n",
      "Epoch: 279\n",
      "Train loss: 0.363\n",
      "Test loss: 0.242\n",
      "Epoch: 280\n",
      "Train loss: 0.357\n",
      "Test loss: 0.247\n",
      "Epoch: 281\n",
      "Train loss: 0.36\n",
      "Test loss: 0.239\n",
      "Epoch: 282\n",
      "Train loss: 0.36\n",
      "Test loss: 0.239\n",
      "Epoch: 283\n",
      "Train loss: 0.357\n",
      "Test loss: 0.238\n",
      "Epoch: 284\n",
      "Train loss: 0.356\n",
      "Test loss: 0.239\n",
      "Epoch: 285\n",
      "Train loss: 0.352\n",
      "Test loss: 0.236\n",
      "Epoch: 286\n",
      "Train loss: 0.354\n",
      "Test loss: 0.241\n",
      "Epoch: 287\n",
      "Train loss: 0.355\n",
      "Test loss: 0.238\n",
      "Epoch: 288\n",
      "Train loss: 0.351\n",
      "Test loss: 0.244\n",
      "Epoch: 289\n",
      "Train loss: 0.349\n",
      "Test loss: 0.239\n",
      "Epoch: 290\n",
      "Train loss: 0.348\n",
      "Test loss: 0.238\n",
      "Epoch: 291\n",
      "Train loss: 0.348\n",
      "Test loss: 0.246\n",
      "Epoch: 292\n",
      "Train loss: 0.346\n",
      "Test loss: 0.244\n",
      "Epoch: 293\n",
      "Train loss: 0.347\n",
      "Test loss: 0.244\n",
      "Epoch: 294\n",
      "Train loss: 0.345\n",
      "Test loss: 0.241\n",
      "Epoch: 295\n",
      "Train loss: 0.342\n",
      "Test loss: 0.238\n",
      "Epoch: 296\n",
      "Train loss: 0.342\n",
      "Test loss: 0.236\n",
      "Epoch: 297\n",
      "Train loss: 0.342\n",
      "Test loss: 0.24\n",
      "Epoch: 298\n",
      "Train loss: 0.341\n",
      "Test loss: 0.243\n",
      "Epoch: 299\n",
      "Train loss: 0.337\n",
      "Test loss: 0.251\n",
      "Epoch: 300\n",
      "Train loss: 0.337\n",
      "Test loss: 0.241\n",
      "Epoch: 301\n",
      "Train loss: 0.335\n",
      "Test loss: 0.244\n",
      "Epoch: 302\n",
      "Train loss: 0.334\n",
      "Test loss: 0.244\n",
      "Epoch: 303\n",
      "Train loss: 0.336\n",
      "Test loss: 0.243\n",
      "Epoch: 304\n",
      "Train loss: 0.333\n",
      "Test loss: 0.235\n",
      "Epoch: 305\n",
      "Train loss: 0.332\n",
      "Test loss: 0.235\n",
      "Epoch: 306\n",
      "Train loss: 0.332\n",
      "Test loss: 0.236\n",
      "Epoch: 307\n",
      "Train loss: 0.328\n",
      "Test loss: 0.239\n",
      "Epoch: 308\n",
      "Train loss: 0.329\n",
      "Test loss: 0.242\n",
      "Epoch: 309\n",
      "Train loss: 0.328\n",
      "Test loss: 0.234\n",
      "Epoch: 310\n",
      "Train loss: 0.323\n",
      "Test loss: 0.233\n",
      "Epoch: 311\n",
      "Train loss: 0.322\n",
      "Test loss: 0.236\n",
      "Epoch: 312\n",
      "Train loss: 0.321\n",
      "Test loss: 0.242\n",
      "Epoch: 313\n",
      "Train loss: 0.318\n",
      "Test loss: 0.239\n",
      "Epoch: 314\n",
      "Train loss: 0.318\n",
      "Test loss: 0.235\n",
      "Epoch: 315\n",
      "Train loss: 0.321\n",
      "Test loss: 0.244\n",
      "Epoch: 316\n",
      "Train loss: 0.317\n",
      "Test loss: 0.235\n",
      "Epoch: 317\n",
      "Train loss: 0.314\n",
      "Test loss: 0.24\n",
      "Epoch: 318\n",
      "Train loss: 0.311\n",
      "Test loss: 0.235\n",
      "Epoch: 319\n",
      "Train loss: 0.314\n",
      "Test loss: 0.235\n",
      "Epoch: 320\n",
      "Train loss: 0.312\n",
      "Test loss: 0.235\n",
      "Epoch: 321\n",
      "Train loss: 0.309\n",
      "Test loss: 0.235\n",
      "Epoch: 322\n",
      "Train loss: 0.308\n",
      "Test loss: 0.249\n",
      "Epoch: 323\n",
      "Train loss: 0.307\n",
      "Test loss: 0.238\n",
      "Epoch: 324\n",
      "Train loss: 0.304\n",
      "Test loss: 0.235\n",
      "Epoch: 325\n",
      "Train loss: 0.303\n",
      "Test loss: 0.242\n",
      "Epoch: 326\n",
      "Train loss: 0.304\n",
      "Test loss: 0.23\n",
      "Epoch: 327\n",
      "Train loss: 0.302\n",
      "Test loss: 0.24\n",
      "Epoch: 328\n",
      "Train loss: 0.303\n",
      "Test loss: 0.228\n",
      "Epoch: 329\n",
      "Train loss: 0.303\n",
      "Test loss: 0.24\n",
      "Epoch: 330\n",
      "Train loss: 0.298\n",
      "Test loss: 0.23\n",
      "Epoch: 331\n",
      "Train loss: 0.297\n",
      "Test loss: 0.236\n",
      "Epoch: 332\n",
      "Train loss: 0.297\n",
      "Test loss: 0.239\n",
      "Epoch: 333\n",
      "Train loss: 0.297\n",
      "Test loss: 0.236\n",
      "Epoch: 334\n",
      "Train loss: 0.294\n",
      "Test loss: 0.231\n",
      "Epoch: 335\n",
      "Train loss: 0.29\n",
      "Test loss: 0.231\n",
      "Epoch: 336\n",
      "Train loss: 0.289\n",
      "Test loss: 0.241\n",
      "Epoch: 337\n",
      "Train loss: 0.287\n",
      "Test loss: 0.237\n",
      "Epoch: 338\n",
      "Train loss: 0.285\n",
      "Test loss: 0.232\n",
      "Epoch: 339\n",
      "Train loss: 0.285\n",
      "Test loss: 0.236\n",
      "Epoch: 340\n",
      "Train loss: 0.285\n",
      "Test loss: 0.233\n",
      "Epoch: 341\n",
      "Train loss: 0.284\n",
      "Test loss: 0.229\n",
      "Epoch: 342\n",
      "Train loss: 0.285\n",
      "Test loss: 0.231\n",
      "Epoch: 343\n",
      "Train loss: 0.282\n",
      "Test loss: 0.241\n",
      "Epoch: 344\n",
      "Train loss: 0.282\n",
      "Test loss: 0.232\n",
      "Epoch: 345\n",
      "Train loss: 0.281\n",
      "Test loss: 0.232\n",
      "Epoch: 346\n",
      "Train loss: 0.276\n",
      "Test loss: 0.233\n",
      "Epoch: 347\n",
      "Train loss: 0.276\n",
      "Test loss: 0.231\n",
      "Epoch: 348\n",
      "Train loss: 0.278\n",
      "Test loss: 0.235\n",
      "Epoch: 349\n",
      "Train loss: 0.276\n",
      "Test loss: 0.23\n",
      "Epoch: 350\n",
      "Train loss: 0.276\n",
      "Test loss: 0.228\n",
      "Epoch: 351\n",
      "Train loss: 0.274\n",
      "Test loss: 0.235\n",
      "Epoch: 352\n",
      "Train loss: 0.27\n",
      "Test loss: 0.229\n",
      "Epoch: 353\n",
      "Train loss: 0.273\n",
      "Test loss: 0.228\n",
      "Epoch: 354\n",
      "Train loss: 0.265\n",
      "Test loss: 0.233\n",
      "Epoch: 355\n",
      "Train loss: 0.27\n",
      "Test loss: 0.225\n",
      "Epoch: 356\n",
      "Train loss: 0.269\n",
      "Test loss: 0.229\n",
      "Epoch: 357\n",
      "Train loss: 0.265\n",
      "Test loss: 0.229\n",
      "Epoch: 358\n",
      "Train loss: 0.265\n",
      "Test loss: 0.229\n",
      "Epoch: 359\n",
      "Train loss: 0.262\n",
      "Test loss: 0.23\n",
      "Epoch: 360\n",
      "Train loss: 0.261\n",
      "Test loss: 0.23\n",
      "Epoch: 361\n",
      "Train loss: 0.259\n",
      "Test loss: 0.23\n",
      "Epoch: 362\n",
      "Train loss: 0.257\n",
      "Test loss: 0.232\n",
      "Epoch: 363\n",
      "Train loss: 0.256\n",
      "Test loss: 0.227\n",
      "Epoch: 364\n",
      "Train loss: 0.259\n",
      "Test loss: 0.233\n",
      "Epoch: 365\n",
      "Train loss: 0.251\n",
      "Test loss: 0.232\n",
      "Epoch: 366\n",
      "Train loss: 0.257\n",
      "Test loss: 0.229\n",
      "Epoch: 367\n",
      "Train loss: 0.25\n",
      "Test loss: 0.226\n",
      "Epoch: 368\n",
      "Train loss: 0.249\n",
      "Test loss: 0.234\n",
      "Epoch: 369\n",
      "Train loss: 0.251\n",
      "Test loss: 0.229\n",
      "Epoch: 370\n",
      "Train loss: 0.249\n",
      "Test loss: 0.224\n",
      "Epoch: 371\n",
      "Train loss: 0.246\n",
      "Test loss: 0.228\n",
      "Epoch: 372\n",
      "Train loss: 0.248\n",
      "Test loss: 0.234\n",
      "Epoch: 373\n",
      "Train loss: 0.246\n",
      "Test loss: 0.231\n",
      "Epoch: 374\n",
      "Train loss: 0.245\n",
      "Test loss: 0.228\n",
      "Epoch: 375\n",
      "Train loss: 0.243\n",
      "Test loss: 0.225\n",
      "Epoch: 376\n",
      "Train loss: 0.238\n",
      "Test loss: 0.232\n",
      "Epoch: 377\n",
      "Train loss: 0.238\n",
      "Test loss: 0.234\n",
      "Epoch: 378\n",
      "Train loss: 0.24\n",
      "Test loss: 0.229\n",
      "Epoch: 379\n",
      "Train loss: 0.24\n",
      "Test loss: 0.226\n",
      "Epoch: 380\n",
      "Train loss: 0.234\n",
      "Test loss: 0.226\n",
      "Epoch: 381\n",
      "Train loss: 0.237\n",
      "Test loss: 0.225\n",
      "Epoch: 382\n",
      "Train loss: 0.239\n",
      "Test loss: 0.224\n",
      "Epoch: 383\n",
      "Train loss: 0.234\n",
      "Test loss: 0.227\n",
      "Epoch: 384\n",
      "Train loss: 0.233\n",
      "Test loss: 0.218\n",
      "Epoch: 385\n",
      "Train loss: 0.232\n",
      "Test loss: 0.224\n",
      "Epoch: 386\n",
      "Train loss: 0.237\n",
      "Test loss: 0.22\n",
      "Epoch: 387\n",
      "Train loss: 0.229\n",
      "Test loss: 0.23\n",
      "Epoch: 388\n",
      "Train loss: 0.228\n",
      "Test loss: 0.231\n",
      "Epoch: 389\n",
      "Train loss: 0.23\n",
      "Test loss: 0.222\n",
      "Epoch: 390\n",
      "Train loss: 0.226\n",
      "Test loss: 0.226\n",
      "Epoch: 391\n",
      "Train loss: 0.227\n",
      "Test loss: 0.223\n",
      "Epoch: 392\n",
      "Train loss: 0.226\n",
      "Test loss: 0.226\n",
      "Epoch: 393\n",
      "Train loss: 0.223\n",
      "Test loss: 0.23\n",
      "Epoch: 394\n",
      "Train loss: 0.221\n",
      "Test loss: 0.228\n",
      "Epoch: 395\n",
      "Train loss: 0.223\n",
      "Test loss: 0.231\n",
      "Epoch: 396\n",
      "Train loss: 0.217\n",
      "Test loss: 0.225\n",
      "Epoch: 397\n",
      "Train loss: 0.217\n",
      "Test loss: 0.226\n",
      "Epoch: 398\n",
      "Train loss: 0.22\n",
      "Test loss: 0.231\n",
      "Epoch: 399\n",
      "Train loss: 0.224\n",
      "Test loss: 0.223\n",
      "Epoch: 400\n",
      "Train loss: 0.217\n",
      "Test loss: 0.226\n",
      "Epoch: 401\n",
      "Train loss: 0.214\n",
      "Test loss: 0.225\n",
      "Epoch: 402\n",
      "Train loss: 0.215\n",
      "Test loss: 0.223\n",
      "Epoch: 403\n",
      "Train loss: 0.213\n",
      "Test loss: 0.223\n",
      "Epoch: 404\n",
      "Train loss: 0.208\n",
      "Test loss: 0.23\n",
      "Epoch: 405\n",
      "Train loss: 0.212\n",
      "Test loss: 0.22\n",
      "Epoch: 406\n",
      "Train loss: 0.207\n",
      "Test loss: 0.224\n",
      "Epoch: 407\n",
      "Train loss: 0.21\n",
      "Test loss: 0.22\n",
      "Epoch: 408\n",
      "Train loss: 0.21\n",
      "Test loss: 0.218\n",
      "Epoch: 409\n",
      "Train loss: 0.203\n",
      "Test loss: 0.222\n",
      "Epoch: 410\n",
      "Train loss: 0.207\n",
      "Test loss: 0.231\n",
      "Epoch: 411\n",
      "Train loss: 0.204\n",
      "Test loss: 0.221\n",
      "Epoch: 412\n",
      "Train loss: 0.202\n",
      "Test loss: 0.226\n",
      "Epoch: 413\n",
      "Train loss: 0.197\n",
      "Test loss: 0.225\n",
      "Epoch: 414\n",
      "Train loss: 0.2\n",
      "Test loss: 0.223\n",
      "Epoch: 415\n",
      "Train loss: 0.202\n",
      "Test loss: 0.22\n",
      "Epoch: 416\n",
      "Train loss: 0.198\n",
      "Test loss: 0.226\n",
      "Epoch: 417\n",
      "Train loss: 0.197\n",
      "Test loss: 0.225\n",
      "Epoch: 418\n",
      "Train loss: 0.198\n",
      "Test loss: 0.218\n",
      "Epoch: 419\n",
      "Train loss: 0.195\n",
      "Test loss: 0.225\n",
      "Epoch: 420\n",
      "Train loss: 0.195\n",
      "Test loss: 0.222\n",
      "Epoch: 421\n",
      "Train loss: 0.194\n",
      "Test loss: 0.218\n",
      "Epoch: 422\n",
      "Train loss: 0.191\n",
      "Test loss: 0.229\n",
      "Epoch: 423\n",
      "Train loss: 0.191\n",
      "Test loss: 0.227\n",
      "Epoch: 424\n",
      "Train loss: 0.189\n",
      "Test loss: 0.219\n",
      "Epoch: 425\n",
      "Train loss: 0.19\n",
      "Test loss: 0.224\n",
      "Epoch: 426\n",
      "Train loss: 0.189\n",
      "Test loss: 0.225\n",
      "Epoch: 427\n",
      "Train loss: 0.188\n",
      "Test loss: 0.22\n",
      "Epoch: 428\n",
      "Train loss: 0.188\n",
      "Test loss: 0.216\n",
      "Epoch: 429\n",
      "Train loss: 0.184\n",
      "Test loss: 0.22\n",
      "Epoch: 430\n",
      "Train loss: 0.188\n",
      "Test loss: 0.222\n",
      "Epoch: 431\n",
      "Train loss: 0.181\n",
      "Test loss: 0.232\n",
      "Epoch: 432\n",
      "Train loss: 0.185\n",
      "Test loss: 0.215\n",
      "Epoch: 433\n",
      "Train loss: 0.179\n",
      "Test loss: 0.222\n",
      "Epoch: 434\n",
      "Train loss: 0.185\n",
      "Test loss: 0.224\n",
      "Epoch: 435\n",
      "Train loss: 0.184\n",
      "Test loss: 0.227\n",
      "Epoch: 436\n",
      "Train loss: 0.182\n",
      "Test loss: 0.22\n",
      "Epoch: 437\n",
      "Train loss: 0.182\n",
      "Test loss: 0.217\n",
      "Epoch: 438\n",
      "Train loss: 0.177\n",
      "Test loss: 0.221\n",
      "Epoch: 439\n",
      "Train loss: 0.184\n",
      "Test loss: 0.22\n",
      "Epoch: 440\n",
      "Train loss: 0.176\n",
      "Test loss: 0.217\n",
      "Epoch: 441\n",
      "Train loss: 0.179\n",
      "Test loss: 0.218\n",
      "Epoch: 442\n",
      "Train loss: 0.175\n",
      "Test loss: 0.22\n",
      "Epoch: 443\n",
      "Train loss: 0.177\n",
      "Test loss: 0.224\n",
      "Epoch: 444\n",
      "Train loss: 0.174\n",
      "Test loss: 0.224\n",
      "Epoch: 445\n",
      "Train loss: 0.171\n",
      "Test loss: 0.221\n",
      "Epoch: 446\n",
      "Train loss: 0.172\n",
      "Test loss: 0.22\n",
      "Epoch: 447\n",
      "Train loss: 0.167\n",
      "Test loss: 0.22\n",
      "Epoch: 448\n",
      "Train loss: 0.171\n",
      "Test loss: 0.222\n",
      "Epoch: 449\n",
      "Train loss: 0.171\n",
      "Test loss: 0.224\n",
      "Epoch: 450\n",
      "Train loss: 0.166\n",
      "Test loss: 0.221\n",
      "Epoch: 451\n",
      "Train loss: 0.171\n",
      "Test loss: 0.214\n",
      "Epoch: 452\n",
      "Train loss: 0.167\n",
      "Test loss: 0.224\n",
      "Epoch: 453\n",
      "Train loss: 0.171\n",
      "Test loss: 0.222\n",
      "Epoch: 454\n",
      "Train loss: 0.164\n",
      "Test loss: 0.218\n",
      "Epoch: 455\n",
      "Train loss: 0.164\n",
      "Test loss: 0.22\n",
      "Epoch: 456\n",
      "Train loss: 0.166\n",
      "Test loss: 0.223\n",
      "Epoch: 457\n",
      "Train loss: 0.163\n",
      "Test loss: 0.221\n",
      "Epoch: 458\n",
      "Train loss: 0.166\n",
      "Test loss: 0.222\n",
      "Epoch: 459\n",
      "Train loss: 0.162\n",
      "Test loss: 0.22\n",
      "Epoch: 460\n",
      "Train loss: 0.158\n",
      "Test loss: 0.22\n",
      "Epoch: 461\n",
      "Train loss: 0.161\n",
      "Test loss: 0.224\n",
      "Epoch: 462\n",
      "Train loss: 0.161\n",
      "Test loss: 0.223\n",
      "Epoch: 463\n",
      "Train loss: 0.163\n",
      "Test loss: 0.221\n",
      "Epoch: 464\n",
      "Train loss: 0.169\n",
      "Test loss: 0.224\n",
      "Epoch: 465\n",
      "Train loss: 0.161\n",
      "Test loss: 0.217\n",
      "Epoch: 466\n",
      "Train loss: 0.163\n",
      "Test loss: 0.215\n",
      "Epoch: 467\n",
      "Train loss: 0.154\n",
      "Test loss: 0.218\n",
      "Epoch: 468\n",
      "Train loss: 0.155\n",
      "Test loss: 0.219\n",
      "Epoch: 469\n",
      "Train loss: 0.158\n",
      "Test loss: 0.219\n",
      "Epoch: 470\n",
      "Train loss: 0.153\n",
      "Test loss: 0.219\n",
      "Epoch: 471\n",
      "Train loss: 0.154\n",
      "Test loss: 0.216\n",
      "Epoch: 472\n",
      "Train loss: 0.155\n",
      "Test loss: 0.218\n",
      "Epoch: 473\n",
      "Train loss: 0.15\n",
      "Test loss: 0.214\n",
      "Epoch: 474\n",
      "Train loss: 0.154\n",
      "Test loss: 0.215\n",
      "Epoch: 475\n",
      "Train loss: 0.151\n",
      "Test loss: 0.217\n",
      "Epoch: 476\n",
      "Train loss: 0.147\n",
      "Test loss: 0.22\n",
      "Epoch: 477\n",
      "Train loss: 0.146\n",
      "Test loss: 0.217\n",
      "Epoch: 478\n",
      "Train loss: 0.154\n",
      "Test loss: 0.215\n",
      "Epoch: 479\n",
      "Train loss: 0.152\n",
      "Test loss: 0.214\n",
      "Epoch: 480\n",
      "Train loss: 0.147\n",
      "Test loss: 0.218\n",
      "Epoch: 481\n",
      "Train loss: 0.145\n",
      "Test loss: 0.214\n",
      "Epoch: 482\n",
      "Train loss: 0.15\n",
      "Test loss: 0.214\n",
      "Epoch: 483\n",
      "Train loss: 0.143\n",
      "Test loss: 0.224\n",
      "Epoch: 484\n",
      "Train loss: 0.15\n",
      "Test loss: 0.217\n",
      "Epoch: 485\n",
      "Train loss: 0.152\n",
      "Test loss: 0.219\n",
      "Epoch: 486\n",
      "Train loss: 0.142\n",
      "Test loss: 0.213\n",
      "Epoch: 487\n",
      "Train loss: 0.14\n",
      "Test loss: 0.223\n",
      "Epoch: 488\n",
      "Train loss: 0.14\n",
      "Test loss: 0.216\n",
      "Epoch: 489\n",
      "Train loss: 0.144\n",
      "Test loss: 0.212\n",
      "Epoch: 490\n",
      "Train loss: 0.144\n",
      "Test loss: 0.214\n",
      "Epoch: 491\n",
      "Train loss: 0.139\n",
      "Test loss: 0.223\n",
      "Epoch: 492\n",
      "Train loss: 0.14\n",
      "Test loss: 0.22\n",
      "Epoch: 493\n",
      "Train loss: 0.141\n",
      "Test loss: 0.215\n",
      "Epoch: 494\n",
      "Train loss: 0.139\n",
      "Test loss: 0.213\n",
      "Epoch: 495\n",
      "Train loss: 0.137\n",
      "Test loss: 0.217\n",
      "Epoch: 496\n",
      "Train loss: 0.145\n",
      "Test loss: 0.219\n",
      "Epoch: 497\n",
      "Train loss: 0.139\n",
      "Test loss: 0.22\n",
      "Epoch: 498\n",
      "Train loss: 0.14\n",
      "Test loss: 0.218\n",
      "Epoch: 499\n",
      "Train loss: 0.135\n",
      "Test loss: 0.219\n",
      "Epoch: 500\n",
      "Train loss: 0.136\n",
      "Test loss: 0.221\n",
      "Epoch: 501\n",
      "Train loss: 0.136\n",
      "Test loss: 0.217\n",
      "Epoch: 502\n",
      "Train loss: 0.134\n",
      "Test loss: 0.22\n",
      "Epoch: 503\n",
      "Train loss: 0.135\n",
      "Test loss: 0.215\n",
      "Epoch: 504\n",
      "Train loss: 0.134\n",
      "Test loss: 0.217\n",
      "Epoch: 505\n",
      "Train loss: 0.132\n",
      "Test loss: 0.216\n",
      "Epoch: 506\n",
      "Train loss: 0.138\n",
      "Test loss: 0.218\n",
      "Epoch: 507\n",
      "Train loss: 0.129\n",
      "Test loss: 0.219\n",
      "Epoch: 508\n",
      "Train loss: 0.133\n",
      "Test loss: 0.214\n",
      "Epoch: 509\n",
      "Train loss: 0.13\n",
      "Test loss: 0.214\n",
      "Epoch: 510\n",
      "Train loss: 0.133\n",
      "Test loss: 0.216\n",
      "Epoch: 511\n",
      "Train loss: 0.13\n",
      "Test loss: 0.218\n",
      "Epoch: 512\n",
      "Train loss: 0.134\n",
      "Test loss: 0.212\n",
      "Epoch: 513\n",
      "Train loss: 0.13\n",
      "Test loss: 0.213\n",
      "Epoch: 514\n",
      "Train loss: 0.131\n",
      "Test loss: 0.22\n",
      "Epoch: 515\n",
      "Train loss: 0.128\n",
      "Test loss: 0.216\n",
      "Epoch: 516\n",
      "Train loss: 0.13\n",
      "Test loss: 0.224\n",
      "Epoch: 517\n",
      "Train loss: 0.128\n",
      "Test loss: 0.218\n",
      "Epoch: 518\n",
      "Train loss: 0.124\n",
      "Test loss: 0.217\n",
      "Epoch: 519\n",
      "Train loss: 0.123\n",
      "Test loss: 0.214\n",
      "Epoch: 520\n",
      "Train loss: 0.13\n",
      "Test loss: 0.216\n",
      "Epoch: 521\n",
      "Train loss: 0.126\n",
      "Test loss: 0.218\n",
      "Epoch: 522\n",
      "Train loss: 0.123\n",
      "Test loss: 0.217\n",
      "Epoch: 523\n",
      "Train loss: 0.127\n",
      "Test loss: 0.219\n",
      "Epoch: 524\n",
      "Train loss: 0.127\n",
      "Test loss: 0.215\n",
      "Epoch: 525\n",
      "Train loss: 0.119\n",
      "Test loss: 0.215\n",
      "Epoch: 526\n",
      "Train loss: 0.123\n",
      "Test loss: 0.219\n",
      "Epoch: 527\n",
      "Train loss: 0.123\n",
      "Test loss: 0.222\n",
      "Epoch: 528\n",
      "Train loss: 0.123\n",
      "Test loss: 0.221\n",
      "Epoch: 529\n",
      "Train loss: 0.124\n",
      "Test loss: 0.221\n",
      "Epoch: 530\n",
      "Train loss: 0.118\n",
      "Test loss: 0.218\n",
      "Epoch: 531\n",
      "Train loss: 0.121\n",
      "Test loss: 0.216\n",
      "Epoch: 532\n",
      "Train loss: 0.121\n",
      "Test loss: 0.221\n",
      "Epoch: 533\n",
      "Train loss: 0.133\n",
      "Test loss: 0.224\n",
      "Epoch: 534\n",
      "Train loss: 0.119\n",
      "Test loss: 0.216\n",
      "Epoch: 535\n",
      "Train loss: 0.114\n",
      "Test loss: 0.212\n",
      "Epoch: 536\n",
      "Train loss: 0.119\n",
      "Test loss: 0.215\n",
      "Epoch: 537\n",
      "Train loss: 0.121\n",
      "Test loss: 0.218\n",
      "Epoch: 538\n",
      "Train loss: 0.119\n",
      "Test loss: 0.22\n",
      "Epoch: 539\n",
      "Train loss: 0.116\n",
      "Test loss: 0.217\n",
      "Epoch: 540\n",
      "Train loss: 0.122\n",
      "Test loss: 0.222\n",
      "Epoch: 541\n",
      "Train loss: 0.116\n",
      "Test loss: 0.216\n",
      "Epoch: 542\n",
      "Train loss: 0.116\n",
      "Test loss: 0.219\n",
      "Epoch: 543\n",
      "Train loss: 0.118\n",
      "Test loss: 0.215\n",
      "Epoch: 544\n",
      "Train loss: 0.116\n",
      "Test loss: 0.214\n",
      "Epoch: 545\n",
      "Train loss: 0.114\n",
      "Test loss: 0.217\n",
      "Epoch: 546\n",
      "Train loss: 0.116\n",
      "Test loss: 0.217\n",
      "Epoch: 547\n",
      "Train loss: 0.113\n",
      "Test loss: 0.218\n",
      "Epoch: 548\n",
      "Train loss: 0.116\n",
      "Test loss: 0.217\n",
      "Epoch: 549\n",
      "Train loss: 0.111\n",
      "Test loss: 0.225\n",
      "Epoch: 550\n",
      "Train loss: 0.119\n",
      "Test loss: 0.218\n",
      "Epoch: 551\n",
      "Train loss: 0.113\n",
      "Test loss: 0.22\n",
      "Epoch: 552\n",
      "Train loss: 0.115\n",
      "Test loss: 0.209\n",
      "Epoch: 553\n",
      "Train loss: 0.116\n",
      "Test loss: 0.216\n",
      "Epoch: 554\n",
      "Train loss: 0.107\n",
      "Test loss: 0.22\n",
      "Epoch: 555\n",
      "Train loss: 0.109\n",
      "Test loss: 0.217\n",
      "Epoch: 556\n",
      "Train loss: 0.11\n",
      "Test loss: 0.215\n",
      "Epoch: 557\n",
      "Train loss: 0.11\n",
      "Test loss: 0.221\n",
      "Epoch: 558\n",
      "Train loss: 0.112\n",
      "Test loss: 0.213\n",
      "Epoch: 559\n",
      "Train loss: 0.107\n",
      "Test loss: 0.214\n",
      "Epoch: 560\n",
      "Train loss: 0.111\n",
      "Test loss: 0.216\n",
      "Epoch: 561\n",
      "Train loss: 0.113\n",
      "Test loss: 0.217\n",
      "Epoch: 562\n",
      "Train loss: 0.109\n",
      "Test loss: 0.217\n",
      "Epoch: 563\n",
      "Train loss: 0.111\n",
      "Test loss: 0.215\n",
      "Epoch: 564\n",
      "Train loss: 0.107\n",
      "Test loss: 0.217\n",
      "Epoch: 565\n",
      "Train loss: 0.108\n",
      "Test loss: 0.218\n",
      "Epoch: 566\n",
      "Train loss: 0.107\n",
      "Test loss: 0.219\n",
      "Epoch: 567\n",
      "Train loss: 0.11\n",
      "Test loss: 0.219\n",
      "Epoch: 568\n",
      "Train loss: 0.107\n",
      "Test loss: 0.217\n",
      "Epoch: 569\n",
      "Train loss: 0.11\n",
      "Test loss: 0.216\n",
      "Epoch: 570\n",
      "Train loss: 0.107\n",
      "Test loss: 0.217\n",
      "Epoch: 571\n",
      "Train loss: 0.106\n",
      "Test loss: 0.214\n",
      "Epoch: 572\n",
      "Train loss: 0.105\n",
      "Test loss: 0.218\n",
      "Epoch: 573\n",
      "Train loss: 0.107\n",
      "Test loss: 0.217\n",
      "Epoch: 574\n",
      "Train loss: 0.109\n",
      "Test loss: 0.216\n",
      "Epoch: 575\n",
      "Train loss: 0.103\n",
      "Test loss: 0.215\n",
      "Epoch: 576\n",
      "Train loss: 0.103\n",
      "Test loss: 0.216\n",
      "Epoch: 577\n",
      "Train loss: 0.103\n",
      "Test loss: 0.221\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\user\\Desktop\\20221205-fttrans.ipynb 셀 22\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/user/Desktop/20221205-fttrans.ipynb#X30sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m test_error_list \u001b[39m=\u001b[39m []\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/user/Desktop/20221205-fttrans.ipynb#X30sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(epoch_num) : \n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/user/Desktop/20221205-fttrans.ipynb#X30sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     \u001b[39m# epoch 당 train loss\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/user/Desktop/20221205-fttrans.ipynb#X30sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     train_loss \u001b[39m=\u001b[39m train(model, dataloader, optimizer, criterion, device) \n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/user/Desktop/20221205-fttrans.ipynb#X30sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     test_error \u001b[39m=\u001b[39m evaluate(model, testloader, device)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/user/Desktop/20221205-fttrans.ipynb#X30sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     train_loss_list\u001b[39m.\u001b[39mappend(train_loss)\n",
      "\u001b[1;32mc:\\Users\\user\\Desktop\\20221205-fttrans.ipynb 셀 22\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(model, dataloader, optimizer, criterion, device)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/user/Desktop/20221205-fttrans.ipynb#X30sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m output \u001b[39m=\u001b[39m model(x_categ, x_numer)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/user/Desktop/20221205-fttrans.ipynb#X30sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m loss \u001b[39m=\u001b[39m criterion(output, y_train) \u001b[39m# batch 에 대한 loss\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/user/Desktop/20221205-fttrans.ipynb#X30sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/user/Desktop/20221205-fttrans.ipynb#X30sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/user/Desktop/20221205-fttrans.ipynb#X30sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m epoch_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem() \u001b[39m# 이번 epoch 에서 loss 총합\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\lib\\site-packages\\torch\\_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[0;32m    486\u001b[0m     )\n\u001b[1;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[0;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[0;32m    489\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    192\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    194\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    195\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    196\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 197\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    198\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[0;32m    199\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_loss_list = []\n",
    "test_error_list = []\n",
    "\n",
    "for epoch in range(epoch_num) : \n",
    "    # epoch 당 train loss\n",
    "    train_loss = train(model, dataloader, optimizer, criterion, device) \n",
    "    test_error = evaluate(model, testloader, device)\n",
    "    \n",
    "    train_loss_list.append(train_loss)\n",
    "    test_error_list.append(test_error)\n",
    "    \n",
    "    print(f'Epoch: {epoch}')\n",
    "    print(f'Train loss: {round(train_loss, 3)}')\n",
    "    print(f'Test loss: {round(test_error, 3)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), '/Users/user/Desktop/20221206_fttrans_status.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set: Accuracy: 79.31%\n"
     ]
    }
   ],
   "source": [
    "# 커스텀 테스트셋 구성\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "# Dataset 상속\n",
    "import numpy as np \n",
    "class Customvalidset(Dataset): \n",
    "    def __init__(self):\n",
    "        self.x_data = x_categ_valid\n",
    "        self.x2_data = x_numer_valid\n",
    "        self.y_data = valid_target_x\n",
    "\n",
    "  # 총 데이터의 개수를 리턴\n",
    "    def __len__(self): \n",
    "        return len(self.x_data)\n",
    "\n",
    "  # 인덱스를 입력받아 그에 맵핑되는 입출력 데이터를 파이토치의 Tensor 형태로 리턴\n",
    "    def __getitem__(self, idx): \n",
    "        x = torch.Tensor(self.x_data[idx]) # categ\n",
    "        x2 = torch.Tensor(self.x2_data[idx]) # numeric\n",
    "        y = torch.Tensor(self.y_data.reshape(len(self.y_data), 1))[idx]\n",
    "        return (x, x2), y\n",
    "    \n",
    "validset = Customvalidset()\n",
    "validloader = DataLoader(validset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "# 테스트셋에서 모델 성능 평가 \n",
    "model.eval() \n",
    "\n",
    "correct = 0 \n",
    "valid = 1\n",
    "for batch_idx, samples in enumerate(validloader) : \n",
    "    x_valid, y_valid = samples\n",
    "    x_categ_valid, x_numer_valid = x_valid # tuple 언패킹\n",
    "    x_categ_valid = x_categ_valid.type(torch.IntTensor)\n",
    "    \n",
    "    x_categ_valid = x_categ_valid.to(device)\n",
    "    x_numer_valid = x_numer_valid.to(device)\n",
    "    y_valid = y_valid.to(device)\n",
    "    \n",
    "    output = model(x_categ_valid, x_numer_valid)\n",
    "    prediction = torch.Tensor([0 if x <= 0.5 else 1 for x in output.data]).to(device)\n",
    "\n",
    "    correct += prediction.eq(torch.flatten(y_valid)).sum()\n",
    "    \n",
    "    valid += 1\n",
    "    \n",
    "print('Validation set: Accuracy: {:.2f}%'.format(100*correct/len(validloader.dataset)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# transformer layer 1 : validation set 77.47% epoch 669, \n",
    "# transformer layer 2: validation set 78.14% epoch 559,\n",
    "# transformer layer 3: validation set 78.95% epoch 800, \n",
    "# transformer alyer 4: validation set 80.66%, train loss=0.225, test loss=0.222 epoch 501, \n",
    "# transformer layer 8 dropout=0.2: validation set 79.31%, epoch 577, train loss 0.103, test loss = 0.221"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 시각화 \n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(range(len(train_loss_list)), train_loss_list, label='Train loss')\n",
    "plt.plot(range(len(test_error_list)), test_error_list, label='Test error')\n",
    "plt.title('FT Transformer train loss and test error: 8 Transformer layer')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a077222d77dfe082b8f1dd562ad70e458ac2ab76993a0b248ab0476e32e9e8dd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
